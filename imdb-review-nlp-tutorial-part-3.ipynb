{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Part 3,4\n",
    "\n",
    "#### ***[코드출처: 오늘코드(박조은 님)](https://github.com/corazzon)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://static.amazon.jobs/teams/53/images/IMDb_Header_Page.jpg?1501027252)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 첫 번째 시도 (average feature vectors)\n",
    "- 튜토리얼2의 코드로 벡터의 평균을 구한다.\n",
    "\n",
    "## 두 번째 시도 (K-means)\n",
    "- Word2Vec은 의미가 관련있는 단어들의 클러스터를 생성하기 때문에 클러스터 내의 단어 유사성을 이용하는 것이다.\n",
    "- 이런식으로 벡터를 그룹화 하는 것을 'Vector Quantization'(벡터 양자화)라고 한다.\n",
    "- 이를 위해서는 K-means와 같은 클러스터링 알고리즘을 사용하여 클러스터라는 단어의 중심을 찾아야 한다.\n",
    "- 비지도 학습인 K-means를 통해 클러스터링을 하고 지도학습인 랜덤포레스트로 리뷰가 추천인지 아닌지를 예측한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/nlp-dataset/labeledTrainData.tsv\", header = 0,\n",
    "                      delimiter = \"\\t\", quoting = 3)\n",
    "df_test = pd.read_csv(\"../input/nlp-dataset/testData.tsv\", header = 0,\n",
    "                      delimiter = \"\\t\", quoting = 3)\n",
    "df_unlabeled = pd.read_csv(\"../input/nlp-dataset/unlabeledTrainData.tsv\", header = 0,\n",
    "                      delimiter = \"\\t\", quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleWord2VecUtility(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def review_to_wordlist(review, remove_stopwords=False):\n",
    "        # 1. HTML 제거\n",
    "        review_text = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "        # 2. 특수문자를 공백으로 바꿔줌\n",
    "        review_text = re.sub('[^a-zA-Z]', ' ', review_text)\n",
    "        # 3. 소문자로 변환 후 나눈다.\n",
    "        words = review_text.lower().split()\n",
    "        # 4. 불용어 제거\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words('english'))\n",
    "            words = [w for w in words if not w in stops]\n",
    "        # 5. 어간추출\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        # 6. 리스트 형태로 반환\n",
    "        return(words)\n",
    "\n",
    "    @staticmethod\n",
    "    def review_to_join_words( review, remove_stopwords=False ):\n",
    "        words = KaggleWord2VecUtility.review_to_wordlist(\\\n",
    "            review, remove_stopwords=False)\n",
    "        join_words = ' '.join(words)\n",
    "        return join_words\n",
    "\n",
    "    @staticmethod\n",
    "    def review_to_sentences( review, remove_stopwords=False ):\n",
    "        # punkt tokenizer를 로드한다.\n",
    "        \"\"\"\n",
    "        이 때, pickle을 사용하는데\n",
    "        pickle을 통해 값을 저장하면 원래 변수에 연결 된 참조값 역시 저장된다.\n",
    "        저장된 pickle을 다시 읽으면 변수에 연결되었던\n",
    "        모든 레퍼런스가 계속 참조 상태를 유지한다.\n",
    "        \"\"\"\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        # 1. nltk tokenizer를 사용해서 단어로 토큰화 하고 공백 등을 제거한다.\n",
    "        raw_sentences = tokenizer.tokenize(review.strip())\n",
    "        # 2. 각 문장을 순회한다.\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            # 비어있다면 skip\n",
    "            if len(raw_sentence) > 0:\n",
    "                # 태그제거, 알파벳문자가 아닌 것은 공백으로 치환, 불용어제거\n",
    "                sentences.append(\\\n",
    "                    KaggleWord2VecUtility.review_to_wordlist(\\\n",
    "                    raw_sentence, remove_stopwords))\n",
    "        return sentences\n",
    "\n",
    "\n",
    "    # 참고 : https://gist.github.com/yong27/7869662\n",
    "    # http://www.racketracer.com/2016/07/06/pandas-in-parallel/\n",
    "    # 속도 개선을 위해 멀티 스레드로 작업하도록\n",
    "    @staticmethod\n",
    "    def _apply_df(args):\n",
    "        df, func, kwargs = args\n",
    "        return df.apply(func, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_by_multiprocessing(df, func, **kwargs):\n",
    "        # 키워드 항목 중 workers 파라메터를 꺼냄\n",
    "        workers = kwargs.pop('workers')\n",
    "        # 위에서 가져온 workers 수로 프로세스 풀을 정의\n",
    "        pool = Pool(processes=workers)\n",
    "        # 실행할 함수와 데이터프레임을 워커의 수 만큼 나눠 작업\n",
    "        result = pool.map(KaggleWord2VecUtility._apply_df, [(d, func, kwargs)\n",
    "                for d in np.array_split(df, workers)])\n",
    "        pool.close()\n",
    "        # 작업 결과를 합쳐서 반환\n",
    "        return pd.concat(result)\n",
    "    \n",
    "    \n",
    "# KaggleWord2VecUtility를 class로 생성하여 사용 \n",
    "# 코드 출처: https://github.com/corazzon/KaggleStruggle/blob/master/word2vec-nlp-tutorial/KaggleWord2VecUtility.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:314: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:389: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for review in df_train[\"review\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(\n",
    "    review, remove_stopwords = False)\n",
    "    \n",
    "# KaggleWord2VecUtility을 사용하여 train 데이터를 정제해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:389: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:389: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:389: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:314: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:389: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:389: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "for review in df_unlabeled[\"review\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(\n",
    "    review, remove_stopwords = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7fd52946d208>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = 300 # 문자 벡터 차원 수 (size)\n",
    "min_word_count = 40 # 최소 문자 수 (min_count)\n",
    "num_workers = 4 # 병렬 처리 스레드 수 (workers)\n",
    "context = 10 # 문자열 창 크기 (window)\n",
    "downsampling = 1e-3 # 문자 빈도 수 Downsample (sample)\n",
    "\n",
    "model = Word2Vec(sentences, workers = num_workers,\n",
    "                 size = num_features, min_count = min_word_count,\n",
    "                 window = context, sample = downsampling)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 숫자로 단어를 표현\n",
    "# Word2Vec 모델은 어휘의 각 단어에 대한 feature 벡터로 구성되며\n",
    "# 'syn0'이라는 넘파이 배열로 저장된다.\n",
    "# syn0의 행 수는 모델 어휘의 단어 수\n",
    "# 컬럼 수는 part 2에서 설정한 피처 벡터의 크기\n",
    "type(model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11986, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"flower\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.028998  ,  0.8449268 ,  0.061165  ,  0.13225688,  0.06006935,\n",
       "       -0.33765045, -0.1157001 , -0.6477264 ,  0.26683876,  0.564404  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"flower\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means 클러스터링으로 데이터 묶기\n",
    "- [k-평균 알고리즘 - 위키백과, 우리 모두의 백과사전](https://ko.wikipedia.org/wiki/K-%ED%8F%89%EA%B7%A0_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)\n",
    "- 클러스터링은 비지도 학습 기법\n",
    "- 클러스터링은 유사성 등 개념에 기초해 몇몇 그룹으로 분류하는 기법\n",
    "- 클러스터링의 목적은 샘플(실수로 구성된 n차원의 벡터)을 내부적으로는 비슷하지만 외부적으로 공통 분모가 없는 여러 그룹으로 묶는 것 \n",
    "- 특정 차원의 범위가 다른 차원과 차이가 크면 클러스터링 하기 전에 스케일을 조정해야 한다.\n",
    "\n",
    "   1. 최초 센트로이드(centroid)(중심점)로 K개의 벡터를 무작위로 선정한다.\n",
    "   2. 각 샘플을 그 위치에서 가장 가까운 센트로이드에 할당한다.\n",
    "   3. 센트로이드의 위치를 재계산한다.\n",
    "   4. 센트로이드가 더 이상 움직이지 않을 때까지 2와 3을 반복한다.\n",
    "   \n",
    "참고: [책]모두의 데이터 과학(with 파이썬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  892.3186316490173 seconds.\n"
     ]
    }
   ],
   "source": [
    "# 단어 벡터에서 k-means를 실행하고 일부 클러스터를 찍어본다.\n",
    "start = time.time()\n",
    "\n",
    "# 클러스터의 크기 \"k\"를 어휘 크기의 1/5 이나 평균 5단어로 설정한다.\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "num_clusters = int(num_clusters)\n",
    "\n",
    "# K-means를 정의하고  학습시킨다.\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "# 끝난시간에서 시작시간을 빼서 걸린 시간을 구한다.\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cluster 0\n",
      "['burton']\n",
      "\n",
      " Cluster 1\n",
      "['lean', 'pen', 'scriptwrit', 'stroke', 'bravo', 'congratul', 'polanski', 'historian', 'poirot', 'biopic', 'bunuel', 'bakshi', 'crumb', 'bruno', 'rewrit', 'scarfac', 'rever', 'cameraman', 'filmographi', 'modesti', 'mileston', 'ki', 'ritchi', 'fincher', 'signatur', 'essay', 'hartley', 'consult', 'kazan', 'prolif', 'vern', 'truman', 'renown', 'chabrol', 'weir', 'autobiographi', 'foss', 'biograph', 'lovecraft', 'besson', 'godard', 'scorces', 'peckinpah', 'antonioni', 'hugo', 'gilliam', 'boorman', 'memoir', 'clanci', 'ozon', 'araki', 'solondz', 'werner', 'herzog', 'adamson', 'kitano', 'yuzna', 'alexandr', 'abel', 'visconti', 'tolkien', 'autobiograph', 'capra', 'fleischer', 'cypher', 'somerset', 'tod', 'corbucci', 'hark', 'fassbind', 'maltin', 'mamet', 'attenborough', 'jarmusch', 'bruckheim', 'novic', 'mulholland', 'excalibur', 'ferrara', 'varga', 'vertigo', 'truffaut', 'elia', 'mckenzi', 'maugham', 'angelopoulo', 'capot', 'visionari', 'caruso', 'raoul', 'shear', 'jang', 'rohmer', 'depalma', 'loach', 'hecht', 'selznick', 'mccarey', 'rollin', 'gregg', 'ridley', 'pyun', 'ernst', 'duk', 'mattei', 'pasolini', 'hodg', 'molina', 'founder', 'zemecki', 'francoi', 'donner', 'kusturica', 'semin', 'jeunet', 'bogdanovich', 'audiard', 'uel', 'schumach', 'alejandro', 'milius', 'wellman', 'ulmer', 'tsui', 'hellman', 'greenaway', 'wender', 'toback', 'becker', 'tarkovski', 'kerrigan', 'minghella', 'ingmar', 'yimou', 'labut', 'vonnegut', 'dp', 'sturg', 'siodmak', 'lelouch', 'damian', 'nakata', 'steinbeck', 'wynorski', 'pseudonym', 'roeg', 'tourneur', 'levinson', 'linklat', 'oi', 'enzo', 'mankiewicz', 'hanek', 'castellari', 'crichton', 'jodorowski', 'harron', 'novella', 'maestro', 'iglesia', 'cukor', 'harlin', 'vidor', 'kasdan', 'sayl', 'erich', 'dahl', 'donen', 'hammett', 'renni', 'orwel', 'demm', 'sergei', 'vincenzo', 'friedman', 'vittorio', 'lerner', 'krabb', 'melvill', 'wook', 'margher', 'alberto']\n",
      "\n",
      " Cluster 2\n",
      "['paus']\n",
      "\n",
      " Cluster 3\n",
      "['forest', 'jungl', 'cave']\n",
      "\n",
      " Cluster 4\n",
      "['won']\n",
      "\n",
      " Cluster 5\n",
      "['revel', 'philosoph', 'ambigu', 'analysi']\n",
      "\n",
      " Cluster 6\n",
      "['whilst', 'weather', 'panic', 'centr', 'parad', 'thru', 'sweep', 'traffic', 'fog', 'dodg', 'mount', 'flood', 'sunni', 'seedi', 'tokyo', 'carniv', 'wax', 'daylight', 'roam', 'amidst', 'volcano', 'decay', 'sleepi', 'desol', 'tropic', 'graveyard', 'morgu', 'thunder', 'crumbl', 'dam', 'dusti', 'loung', 'caf', 'downtown', 'forebod', 'pond', 'muddi', 'mist', 'shadowi', 'loom', 'wasteland', 'indoor', 'snowi', 'cramp', 'terrain', 'ici', 'darken', 'picturesqu', 'silhouett', 'breez', 'maze', 'foggi', 'scenic', 'ember', 'atop', 'barren', 'stormi', 'dilapid']\n",
      "\n",
      " Cluster 7\n",
      "['neat']\n",
      "\n",
      " Cluster 8\n",
      "['dare', 'toler', 'disguis', 'nowaday', 'guarante', 'conceiv', 'label', 'qualifi', 'dismiss', 'mislead', 'goer', 'disgrac', 'remad', 'classifi', 'summar', 'forgiven', 'fathom', 'categor']\n",
      "\n",
      " Cluster 9\n",
      "['practic', 'therefor', 'safe', 'strict', 'necessarili', 'firm', 'automat', 'approv', 'exclus', 'oblig', 'restrict', 'constitut', 'pc', 'procedur', 'violat', 'readili', 'demograph', 'permit', 'incom', 'denomin']\n"
     ]
    }
   ],
   "source": [
    "# 각 어휘 단어를 클러스터 번호에 매핑되게 word/index 사전을 만든다.\n",
    "idx = list(idx)\n",
    "names = model.wv.index2word\n",
    "word_centroid_map = {names[i]: idx[i] for i in range(len(names))}\n",
    "# word_cenetroid_map = dict(zip(model.wv.index2word, idx))\n",
    "\n",
    "# 첫번째 클러스터의 처음 10개를 출력\n",
    "for cluster in range(0, 10):\n",
    "    # 클러스터 번호를 출력\n",
    "    print(\"\\n Cluster {}\".format(cluster))\n",
    "    \n",
    "    # 클러스터 번호와 클러스터에 있는 단어를 찍는다.\n",
    "    words = []\n",
    "    for i in range(0, len(list(word_centroid_map.values()))):\n",
    "        if(list(word_centroid_map.values())[i] == cluster):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n판다스로 데이터 프레임 형태의 데이터로 읽어온다.\\n\\n그리고 이전 튜토리얼에서 했던 것 처럼 clean_train_review와\\nclean_test_review로 텍스트를 정제한다.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "판다스로 데이터 프레임 형태의 데이터로 읽어온다.\n",
    "\n",
    "그리고 이전 튜토리얼에서 했던 것 처럼 clean_train_review와\n",
    "clean_test_review로 텍스트를 정제한다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_reviews = []\n",
    "for review in df_train[\"review\"]:\n",
    "    clean_train_reviews.append(\n",
    "        KaggleWord2VecUtility.review_to_wordlist(review,\n",
    "                                                remove_stopwords = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_reviews = []\n",
    "for review in df_test[\"review\"]:\n",
    "    clean_test_reviews.append(\n",
    "        KaggleWord2VecUtility.review_to_wordlist(review,\n",
    "                                                remove_stopwords = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bag of centroids 생성\n",
    "# 속도를 위해 centroid 학습 세트 bag을 미리 할당한다.\n",
    "train_centroids = np.zeros((df_train[\"review\"].size, num_clusters),\n",
    "                          dtype = \"float32\")\n",
    "\n",
    "train_centroids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid는 두 클러스터의 중심점을 정의 한 다음 중심점의 거리를 측정한 것\n",
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    \n",
    "    # 클러스터의 수는 word/centroid map에서 가장 높은 클러스트 인덱스와 같다.\n",
    "    num_centroids = max(word_centroid_map.values())+1\n",
    "    \n",
    "    # 속도를 위해 bag of centroids vector를 미리 할당한다.\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype = \"float32\")\n",
    "    \n",
    "    # 루프를 돌며 단어가 word_centroid_map에 있다면\n",
    "    # 해당되는 클러스터의 수를 하나씩 증가시켜 준다.\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "            \n",
    "    # bag of centroids를 반환한다.\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 리뷰를 bags of centroids로 변환한다.\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review,\n",
    "                                                      word_centroid_map)\n",
    "    counter += 1\n",
    "\n",
    "# 테스트 리뷰도 같은 방법으로 반복해 준다.\n",
    "test_centroids = np.zeros((df_test[\"review\"].size, num_clusters),\n",
    "                         dtype = \"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review,\n",
    "                                                      word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.4 s, sys: 51.4 ms, total: 52.4 s\n",
      "Wall time: 52.4 s\n"
     ]
    }
   ],
   "source": [
    "# RandomForest를 사용하여 학습시키고 예측\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# train 데이터의 레이블을 통해 학습시키고 예측한다.\n",
    "%time rf = rf.fit(train_centroids, df_train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 51s, sys: 681 ms, total: 5min 52s\n",
      "Wall time: 5min 52s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "%time score = np.mean(cross_val_score(rf, train_centroids,\\\n",
    "                                df_train[\"sentiment\"],\\\n",
    "                                      cv = 10,\\\n",
    "                                      scoring = \"roc_auc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.68 s, sys: 1.35 ms, total: 1.68 s\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%time result = rf.predict(test_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.921640448"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data = {\"id\": df_test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"./submit_BagofCentroids_{:.3f}.csv\".format(score),index = False,quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 왜 이 튜토리얼에서는 BoW가 더 좋은 결과를 가져올까?\n",
    "\n",
    "- 벡터를 평균화하고 centroids를 사용하면 단어 순서가 어긋나며 BoW 개념과 매우 비슷하다. 성능이(표준 오차의 범위 내에서)비슷하기 때문에 튜토리얼 1, 2, 3이 동등한 결과를 가져온다.\n",
    "\n",
    "- 1) Word2Vec을 더 많은 텍스트로 학습시키면 성능이 좋아진다. Google의 결과는 10억 단어가 넘는 corpus에서 배운 단어 벡터를 기반으로 한다. 학습레이블이 있거나 레이블이 없는 학습 세트는 단지 대력 천팔백만 단어 정도다. 편의상 Word2Vec은 Google의 원래 C도구에서 출력되는 사전 학습된 모델을 로드하는 기능을 제공하기 때문에 C로 모델을 학습한 다음 Python으로 가져올 수도 있다.\n",
    "\n",
    "- 2) 출판 된 자료들에서 분산 워드 벡터 기술은 BoW 모델보다 우수한 것으로 나타났다. 이 논문에서는 IMDb 데이터 집합에 단락 벡터(Paragraph Vector)라는 알고리즘을 사용하여 현재까지의 최첨단 결과 중 일부를 생성한다. 단락 벡터는 단어 순서 정보를 보존하는 반면 벡터 평균화 및 클러스터링은 단어 순서를 잃어 버리기 때문에 여기에서 시도하는 방식보다 부분적으로 좋다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
